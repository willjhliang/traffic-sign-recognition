{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willjhliang/traffic-sign-recognition/blob/main/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRrDti8n2Vw8"
      },
      "outputs": [],
      "source": [
        "# Download dataset from github repo\n",
        "!git clone --quiet https://github.com/willjhliang/traffic-sign-recognition.git\n",
        "!mv traffic-sign-recognition/data .\n",
        "!rm -r traffic-sign-recognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from copy import deepcopy\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.base import clone\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "import xgboost as xgb\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "from torch import Tensor\n",
        "import torchvision\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report\n",
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "bDwc2e3-4pvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 36                  # Number of classes\n",
        "S = 32                  # Size of image, dimension is (s, s, 3)\n",
        "class_size = 320        # Number of images per class\n",
        "validation_ratio = 0.1  # Proportion of training data to set aside for validation\n",
        "test_ratio = 0.1        # Proportion of full training data to set aside for testing\n",
        "\n",
        "random_seed = 19104     # Seed all random operations to ensure reproducibility\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)"
      ],
      "metadata": {
        "id": "8auRbcGP5jJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "FaaChV3Way94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(datapath):\n",
        "    \"\"\"Loads images from files and performs basic pre-processing.\"\"\"\n",
        "    data = {}\n",
        "    for k in range(K):\n",
        "        data[k] = []\n",
        "\n",
        "    for f in os.listdir(datapath):\n",
        "        k = int(f[:3])  # Get label from filename\n",
        "        img = Image.open(os.path.join(datapath, f))\n",
        "        img = np.asarray(img) / 255  # Set pixel values to [0, 1]\n",
        "        if len(data[k]) < class_size:\n",
        "            data[k].append(img)\n",
        "\n",
        "    train_data, test_data = {}, {}\n",
        "    for k in range(K):\n",
        "        random.shuffle(data[k])\n",
        "        split = int(len(data[k]) * test_ratio)\n",
        "        train_data[k] = data[k][split:]\n",
        "        test_data[k] = data[k][:split]\n",
        "    \n",
        "    return train_data, test_data"
      ],
      "metadata": {
        "id": "Gr29Pwh625Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = load_data('data/filtered_images')\n",
        "labels = pd.read_csv(\"data/filtered_labels.csv\")"
      ],
      "metadata": {
        "id": "Br0-GXCA4rts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "\n",
        "We'll explore the dataset by displaying example images from each class. We also plot the number of images belonging to each class and find that it's extremely variable."
      ],
      "metadata": {
        "id": "MjD_ep2c5HUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(6, 10, figsize=(15, 8))\n",
        "for k, (i, j) in itertools.zip_longest(range(K), list(itertools.product(range(6), range(10))), fillvalue=-1):\n",
        "    axs[i,j].axis('off')\n",
        "    if k >= 0:\n",
        "        axs[i,j].imshow(train_data[k][0])  # Visualize the first image of every class"
      ],
      "metadata": {
        "id": "MeVxBPI864JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(6, 10, figsize=(15, 8))\n",
        "for k, (i, j) in itertools.zip_longest(range(K), list(itertools.product(range(6), range(10))), fillvalue=-1):\n",
        "    axs[i,j].axis('off')\n",
        "    if k >= 0 and len(test_data[k]) > 0:\n",
        "        axs[i,j].imshow(test_data[k][0])  # Visualize the first image of every class in the testing set"
      ],
      "metadata": {
        "id": "nKSJPEi5s0si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_class_dist = [len(train_data[k]) for k in range(K)]\n",
        "test_class_dist = [len(test_data[k]) for k in range(K)]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axs[0].bar(list(range(K)), train_class_dist)\n",
        "axs[1].bar(list(range(K)), test_class_dist);"
      ],
      "metadata": {
        "id": "w3rniKpZ9gNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "To preprocess our data, we'll first augment the classes with fewer image examples. Our augmentation scheme includes cropping, rotation, and brightness changes; note that we don't apply any flips since it violates the symbols on traffic signs.\n",
        "\n",
        "After augmentation, we reshape the data to an array format and store labels as integers."
      ],
      "metadata": {
        "id": "kwHmRzn_WERd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def center_crop(img, center_percentage):\n",
        "    \"\"\"Crops out edges of an image, leaving the center.\"\"\"\n",
        "    width, height, _ = img.shape\n",
        "    width_offset = int(width * (1 - center_percentage) / 2)\n",
        "    height_offset = int(height * (1 - center_percentage) / 2)\n",
        "    img = img[width_offset:width-width_offset, height_offset:height-height_offset]\n",
        "    return img\n",
        "\n",
        "\n",
        "def rotate_img(img, angle):\n",
        "    \"\"\"Rotates an image and replaces empty space with black.\"\"\"\n",
        "    height, width, _ = img.shape\n",
        "    center_x, center_y = (width // 2, height // 2)\n",
        "\n",
        "    rot_mat = cv2.getRotationMatrix2D((center_x, center_y), angle, 1.0)\n",
        "    cos = np.abs(rot_mat[0, 0])\n",
        "    sin = np.abs(rot_mat[0, 1])\n",
        "\n",
        "    new_width = int((height * sin) + (width * cos))\n",
        "    new_height = int((height * cos) + (width * sin))\n",
        "    rot_mat[0, 2] += (new_width / 2) - center_x\n",
        "    rot_mat[1, 2] += (new_height / 2) - center_y\n",
        "\n",
        "    img = cv2.warpAffine(img, rot_mat, (new_width, new_height))\n",
        "    img = cv2.resize(img, (width, height))\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def shift_brightness(img, shift):\n",
        "    \"\"\"Adjusts brightness of all pixels in image.\"\"\"\n",
        "    img = np.clip(img + shift, 0, 1)\n",
        "    return img"
      ],
      "metadata": {
        "id": "ETRb3zMX869y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_img(img):\n",
        "    \"\"\"Augments image with rotation, cropping, and brightness shifts.\"\"\"\n",
        "    rot_angle = random.randint(-20, 20)\n",
        "    crop_center_percentage = random.randint(70, 90) / 100\n",
        "    crop_center_percentage = 0.8\n",
        "    brightness_shift = random.randint(-10, 10) / 100\n",
        "\n",
        "    img = rotate_img(img, rot_angle)\n",
        "    # img = center_crop(img, crop_center_percentage)\n",
        "    # img = shift_brightness(img, brightness_shift)\n",
        "    img = center_crop(img, 0.8)\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "D0LV8uZsCB0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(6, 10, figsize=(15, 8))\n",
        "for k, (i, j) in itertools.zip_longest(range(K), list(itertools.product(range(6), range(10))), fillvalue=-1):\n",
        "    axs[i,j].axis('off')\n",
        "    if k >= 0:\n",
        "        img = augment_img(train_data[k][-1])\n",
        "        axs[i,j].imshow(augment_img(img))"
      ],
      "metadata": {
        "id": "aslUcb0wNSAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_k_size = max([len(train_data[k]) for k in range(K)])\n",
        "for k in range(K):\n",
        "    k_size = len(train_data[k])\n",
        "    for i in range(max_k_size - k_size):  # Add augmented images until we have class_size images\n",
        "        train_data[k].append(augment_img(train_data[k][i % k_size]))"
      ],
      "metadata": {
        "id": "UG-NpBFR8hgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_class_dist = [len(train_data[k]) for k in range(K)]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axs[0].bar(list(range(K)), train_class_dist)\n",
        "axs[1].bar(list(range(K)), aug_class_dist);"
      ],
      "metadata": {
        "id": "nt5sG5QiHtzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data):\n",
        "    \"\"\"Converts image-label data from map to numpy arrays.\"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    for k in range(K):\n",
        "        for i in data[k]:\n",
        "            i = cv2.resize(i, (S, S))\n",
        "            X.append(np.swapaxes(i, 0, -1))\n",
        "            y.append(k)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    shuffled_indices = np.random.permutation(len(X))\n",
        "    X = X[shuffled_indices]\n",
        "    y = y[shuffled_indices]\n",
        "    X_flattened = np.reshape(X, (X.shape[0], -1))\n",
        "    \n",
        "    return X, X_flattened, y"
      ],
      "metadata": {
        "id": "4UVlLK8c3HXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_train_flattened, y_train = prepare_data(train_data)\n",
        "X_test, X_test_flattened, y_test = prepare_data(test_data)"
      ],
      "metadata": {
        "id": "MTT4G6q63fp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_validation(X_train, y_train):\n",
        "    \"\"\"Splits training data into train and validation sets. Used in models below.\"\"\"\n",
        "    val_split = int(X_train.shape[0] * validation_ratio)\n",
        "    X_train, X_val = X_train[val_split:], X_train[:val_split]\n",
        "    y_train, y_val = y_train[val_split:], y_train[:val_split]\n",
        "    return X_train, X_val, y_train, y_val"
      ],
      "metadata": {
        "id": "27w1N04HdH8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction\n",
        "\n",
        "With 32 x 32 features, our models below take a long time to converge. We try both PCA and neural network autoencoders to reduce the feature space before training our sklearn models."
      ],
      "metadata": {
        "id": "xXEGZ3KE_adb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covar_matrix = PCA(n_components=32*32)\n",
        "covar_matrix.fit(X_train_flattened)\n",
        "variance = covar_matrix.explained_variance_ratio_\n",
        "var=np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3)*100)\n",
        "plt.plot(var[:300]);"
      ],
      "metadata": {
        "id": "CLwF2wzM_wyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=130)\n",
        "pca.fit(X_train_flattened)\n",
        "X_train_pca = pca.transform(X_train_flattened)\n",
        "X_test_pca = pca.transform(X_test_flattened)"
      ],
      "metadata": {
        "id": "stcaq1MfBJJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n",
        "\n",
        "The following is a set of models we run on the data. Starting with the most simple baseline K-Nearest Neighbors, we move toward more complex models.\n",
        "1. Baseline KNN\n",
        "2. Adaboost\n",
        "3. Logistic Regression\n",
        "4. Kernelized SVM\n",
        "5. Dense Neural Network\n",
        "6. Convolutional Neural Network\n",
        "\n",
        "We also test two more advanced strategies.\n",
        "1. Autoencoder dimensionality reduction allows us to embed the images in a lower dimensional space, which may lead to stronger classification performance by simpler models.\n",
        "2. Transfer learning with a CNN allows us to adapt weights from pre-trained networks to our traffic sign recognition problem."
      ],
      "metadata": {
        "id": "zUuXolGt5JZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline KNN\n",
        "\n",
        "Train a baseline K-Nearest Neighbors models to classify traffic sign images. Use 5-Fold cross validation to determine the best value of K."
      ],
      "metadata": {
        "id": "7WmDtprV5NqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5)\n",
        "\n",
        "\n",
        "def evaluate_kfold(model_base, X_train, y_train):\n",
        "    \"\"\"Evaluates the given model with K-Fold cross validation.\"\"\"\n",
        "    total_acc = 0\n",
        "    for train_index, val_index in kf.split(X_train): # Iterate through folds\n",
        "       # Split data into training data and validation data\n",
        "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        " \n",
        "        # Train model\n",
        "        model = clone(model_base)\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "        total_acc += model.score(X_val_fold, y_val_fold)\n",
        "       \n",
        "    avg_acc = total_acc / 5\n",
        "    return avg_acc\n",
        "\n",
        "\n",
        "def knn(X_train, y_train, X_test, y_test):\n",
        "    k_values = [1, 3, 5, 7, 9, 11, 13, 15]\n",
        "    best_k = -1\n",
        "    best_acc = 0\n",
        "\n",
        "    accs = []\n",
        "    for k_neighbors in tqdm(k_values, leave=False):\n",
        "        avg_acc = evaluate_kfold(KNeighborsClassifier(n_neighbors = k_neighbors), X_train, y_train)\n",
        "        accs.append(avg_acc)\n",
        "        if avg_acc > best_acc:\n",
        "            best_acc = avg_acc\n",
        "            best_k = k_neighbors\n",
        " \n",
        "    # Plot validation scores for each tested k-value\n",
        "    plt.plot(k_values, accs)\n",
        "    plt.show()\n",
        "    print(f\"Optimal k: {best_k}\")\n",
        " \n",
        "    # Train model with the best k value\n",
        "    model = KNeighborsClassifier(n_neighbors=best_k)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    return model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "GokJxe4KGJz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adaboost"
      ],
      "metadata": {
        "id": "M1sX4gyWji8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adaboost(X_train_full, y_train_full, X_test, y_test):\n",
        "    X_train, X_val, y_train, y_val = get_validation(X_train_full, y_train_full)\n",
        "\n",
        "    learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "    best_lr = -1\n",
        "    best_acc = 0\n",
        "\n",
        "    accs = []\n",
        "    for lr in tqdm(learning_rates, leave=False):\n",
        "        model = AdaBoostClassifier(\n",
        "            DecisionTreeClassifier(max_depth=1),\n",
        "            n_estimators=200,\n",
        "            algorithm=\"SAMME.R\",\n",
        "            learning_rate=lr,\n",
        "            random_state=random_seed\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "        acc = model.score(X_val, y_val)\n",
        "        accs.append(acc)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_lr = lr\n",
        "  \n",
        "    plt.plot(accs)\n",
        "    plt.xticks(list(range(len(accs))), [str(lr) for lr in learning_rates])\n",
        "    plt.show()\n",
        "    print(f'Optimal learning rate: {best_lr}')\n",
        "    \n",
        "    model = AdaBoostClassifier(\n",
        "        DecisionTreeClassifier(max_depth=1),\n",
        "        n_estimators=200,\n",
        "        algorithm=\"SAMME.R\",\n",
        "        learning_rate=lr,\n",
        "        random_state=random_seed\n",
        "    )\n",
        "    model.fit(X_train_full, y_train_full)\n",
        " \n",
        "    return model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "gdmToVHvYAvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "17znCaqJ82C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xgboost(X_train_full, y_train_full, X_test, y_test):\n",
        "    X_train, X_val, y_train, y_val = get_validation(X_train_full, y_train_full)\n",
        "\n",
        "    learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "    best_lr = -1\n",
        "    best_acc = 0\n",
        " \n",
        "    accs = []\n",
        "    for lr in tqdm(learning_rates, leave=False):\n",
        "        model = xgb.XGBClassifier(n_estimators=200, max_depth=1, learning_rate=0.1, objective='multi:softmax', booster='gbtree', num_classes=K)\n",
        "        model.fit(X_train, y_train)\n",
        "        acc = model.score(X_val, y_val)\n",
        "        accs.append(acc)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_lr = lr\n",
        "     \n",
        "    plt.plot(accs)\n",
        "    plt.xticks(list(range(len(accs))), [str(lr) for lr in learning_rates])\n",
        "    plt.show()\n",
        "    print(f'Optimal learning rate: {best_lr}')\n",
        "  \n",
        "    model = xgb.XGBClassifier(n_estimators=200, max_depth=1, learning_rate=lr, objective='multi:softmax', booster='gbtree', num_classes=K)\n",
        "    model.fit(X_train_full, y_train_full)\n",
        "    return model.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "UJagudza83b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "fgSsZgmhjr5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X_train_full, y_train_full, X_test, y_test):\n",
        "    X_train, X_val, y_train, y_val = get_validation(X_train_full, y_train_full)\n",
        "\n",
        "    C_values = [0.01, 0.1, 1.0, 10, 100]\n",
        "    best_C = -1\n",
        "    best_acc = 0\n",
        "    \n",
        "    accs = []\n",
        "    for c in tqdm(C_values, leave=False):\n",
        "        model = LogisticRegression(\n",
        "            penalty='l2',\n",
        "            C=c,\n",
        "            multi_class='multinomial',\n",
        "            solver='saga',\n",
        "            max_iter=500\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "        acc = model.score(X_val, y_val)\n",
        "        accs.append(acc)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_C = c\n",
        "        \n",
        "    # Get the best combination\n",
        "    plt.plot(accs)\n",
        "    plt.xticks(list(range(len(accs))), [str(c) for c in C_values])\n",
        "    plt.show()\n",
        "    print(f'Optimal C: {best_C}')\n",
        " \n",
        "    # Train new model with best combination\n",
        "    bestlogModel = LogisticRegression(\n",
        "        penalty='l2',\n",
        "        C=best_C,\n",
        "        multi_class = 'multinomial',\n",
        "        solver='saga',\n",
        "        max_iter=500\n",
        "    )\n",
        "    bestlogModel.fit(X_train_full, y_train_full)\n",
        "    return bestlogModel.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "KoQq0QNcX4T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernelized SVM"
      ],
      "metadata": {
        "id": "kBUqSumgjuwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel_svm(X_train_full, y_train_full, X_test, y_test):\n",
        "    X_train, X_val, y_train, y_val = get_validation(X_train_full, y_train_full)\n",
        "\n",
        "    kernels = ['linear', 'poly', 'rbf']\n",
        "    C_values = [0.01, 0.1, 1, 10, 100]\n",
        "    best_kernel = ''\n",
        "    best_C = -1\n",
        "    best_acc = 0\n",
        "\n",
        "    accs = {}\n",
        "    for kernel in kernels:\n",
        "        accs[kernel] = []\n",
        "    for kernel, c in tqdm(itertools.product(kernels, C_values), leave=False):\n",
        "        model = SVC(kernel=kernel, C=c)\n",
        "        model.fit(X_train, y_train)\n",
        "        acc = model.score(X_val, y_val)\n",
        "        accs[kernel].append(acc)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_C = c\n",
        "            best_kernel = kernel\n",
        "    \n",
        "    best_accs = [max(accs[kernel]) for kernel in kernels]\n",
        "    plt.bar(kernels, best_accs)\n",
        "    plt.xticks(list(range(len(best_accs))), kernels)\n",
        "    plt.show()\n",
        "    print(f'Optimal kernel: {best_kernel}')\n",
        "\n",
        "    model = SVC(kernel=best_kernel, C=best_C)\n",
        "    model.fit(X_train_full, y_train_full)\n",
        "    return model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "iCLRPjXyYJOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense Neural Network"
      ],
      "metadata": {
        "id": "UPCS7NeYjxn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_torch_data(X_train, y_train, X_test, y_test):\n",
        "    X_train, X_val, y_train, y_val = get_validation(X_train, y_train)\n",
        "    train_set = data.TensorDataset(Tensor(X_train), Tensor(y_train))\n",
        "    val_set = data.TensorDataset(Tensor(X_val), Tensor(y_val))\n",
        "    test_set = data.TensorDataset(Tensor(X_test), Tensor(y_test))\n",
        "    train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "    val_loader = data.DataLoader(val_set, batch_size=32, shuffle=True)\n",
        "    test_loader = data.DataLoader(test_set, batch_size=32, shuffle=True)\n",
        "    return train_loader, val_loader, test_loader\n",
        "    \n",
        "\n",
        "def train_network(model, train_loader, val_loader, epochs, lr):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr) \n",
        "\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0\n",
        "        for itr, (image, label) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            y_predicted = model(image)\n",
        "            label = label.long()\n",
        "\n",
        "            loss = criterion(y_predicted, label)\n",
        "            running_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "        train_loss.append(running_loss)\n",
        "        train_acc.append(evaluate_network(model, train_loader))\n",
        "        val_acc.append(evaluate_network(model, val_loader))\n",
        "        print(f'Epoch: {epoch+1:03}, Loss: {running_loss:9.4f}, Train Accuracy: {train_acc[-1]:.4f}, Validation Accuracy: {val_acc[-1]:.4f}')\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    axs[0].plot(train_loss)\n",
        "    axs[1].plot(list(range(epochs)), train_acc, val_acc);\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_network(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for itr, (image, label) in enumerate(dataloader):\n",
        "            outputs = model(image)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.eq(label.reshape(len(label),)).sum() \n",
        "            total += float(len(label))\n",
        "        accuracy = correct / total\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "F7uKqh78ospP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(32 * 32 * 3, 128)\n",
        "        self.layer2 = nn.Linear(128, 64)\n",
        "        self.out_layer = nn.Linear(64, K)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.out_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kXZT0kN1c0JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network"
      ],
      "metadata": {
        "id": "xsattGC-jzYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(32)\n",
        "        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(32)\n",
        "        self.conv_3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(64)\n",
        "        self.conv_4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch_norm_4 = nn.BatchNorm2d(64)\n",
        "        self.dropout_1 = nn.Dropout(0.5)\n",
        "        self.conv_5 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch_norm_5 = nn.BatchNorm2d(64)\n",
        "        self.conv_6 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.batch_norm_6 = nn.BatchNorm2d(64)\n",
        "        self.dropout_2 = nn.Dropout(0.5)\n",
        "        self.linear_1 = nn.Linear(4 * 4 * 64, 128)\n",
        "        self.dropout_3 = nn.Dropout(0.25)\n",
        "        self.linear_2 = nn.Linear(128, K)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.batch_norm_1(self.conv_1(x)))\n",
        "        x = self.relu(self.batch_norm_2(self.conv_2(x)))\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.relu(self.batch_norm_3(self.conv_3(x)))\n",
        "        x = self.relu(self.batch_norm_4(self.conv_4(x)))\n",
        "        x = self.dropout_1(x)\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.relu(self.batch_norm_5(self.conv_5(x)))\n",
        "        x = self.relu(self.batch_norm_6(self.conv_6(x)))\n",
        "        x = self.dropout_2(x)\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.linear_1(x))\n",
        "        x = self.dropout_3(x)\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4CB0H8YxKTnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning"
      ],
      "metadata": {
        "id": "jI9lT-q-kteU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Resnet():\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, K)\n",
        "    return model"
      ],
      "metadata": {
        "id": "LCCV5sIrH-Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VGG16():\n",
        "    model = torchvision.models.vgg16(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    num_ftrs = model.classifier[-1].in_features\n",
        "    model.classifier[-1] = nn.Linear(num_ftrs, K)\n",
        "    return model"
      ],
      "metadata": {
        "id": "0Xq-iQwNfbKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EfficientNet():\n",
        "    model = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    num_ftrs = model.classifier[-1].in_features\n",
        "    model.classifier[-1] = nn.Linear(num_ftrs, K)\n",
        "    return model"
      ],
      "metadata": {
        "id": "IDWWGFp6kW8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation\n",
        "We now run all the models defined above."
      ],
      "metadata": {
        "id": "jLYfefMU_GtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generateConfusionMatrix(y_actual, y_pred):\n",
        "    mat = confusion_matrix(y_actual, y_pred)\n",
        "    plt.figure(figsize = (30, 30))\n",
        "    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels = labels['Name'], yticklabels = labels['Name'])\n",
        "    plt.xlabel('true label')\n",
        "    plt.ylabel('predicted label')\n",
        " \n",
        " \n",
        "def classificationReport(y_actual, y_pred):\n",
        "    print(classification_report(y_actual, y_pred, target_names = labels['Name']))\n",
        " \n",
        "\n",
        "def perClassAccuracy(y_actual, y_pred):\n",
        "    mat = confusion_matrix(y_actual, y_pred)\n",
        "    class_accuracies = mat.diagonal()/(mat.sum(axis = 1))\n",
        "    tablearray = np.column_stack((labels['Name'], class_accuracies))\n",
        "    print(tabulate(tablearray, headers = ['Label', 'Accuracy'], tablefmt = 'fancy_grid'))"
      ],
      "metadata": {
        "id": "ulTw4pvxCNnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('========== KNN ==========')\n",
        "acc = knn(X_train_flattened, y_train, X_test_flattened, y_test)\n",
        "print(f'Test Accuracy: {acc}')"
      ],
      "metadata": {
        "id": "vvxEFB-3iciz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('========== KNN (PCA) ==========')\n",
        "acc = knn(X_train_pca, y_train, X_test_pca, y_test)\n",
        "print(f'Test Accuracy: {acc}')"
      ],
      "metadata": {
        "id": "6g8G3SdJ_LZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('========== Adaboost (PCA) ==========')\n",
        "acc = adaboost(X_train_pca, y_train, X_test_pca, y_test)\n",
        "print(f'Test Accuracy: {acc}')"
      ],
      "metadata": {
        "id": "8_N92jyr_QG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('========== Logistic Regression (PCA) ==========')\n",
        "acc = logistic_regression(X_train_pca, y_train, X_test_pca, y_test)\n",
        "print(f'Test Accuracy: {acc}')"
      ],
      "metadata": {
        "id": "YcuHmemjqoLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('========== Kernelized SVM (PCA) ==========')\n",
        "acc = kernel_svm(X_train_pca, y_train, X_test_pca, y_test)\n",
        "print(f'Test Accuracy: {acc}')"
      ],
      "metadata": {
        "id": "LAElP9LLniup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('========== Dense Neural Network ==========')\n",
        "train_loader, val_loader, test_loader = load_torch_data(X_train_flattened, y_train, X_test_flattened, y_test)\n",
        "model = train_network(NN(), train_loader, val_loader, 30, 1e-3)\n",
        "acc = evaluate_network(model, test_loader)\n",
        "print(f'Test Accuracy: {acc}')"
      ],
      "metadata": {
        "id": "o-ANqgbYpuf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('========== Convolutional Neural Network ==========')\n",
        "train_loader, val_loader, test_loader = load_torch_data(X_train, y_train, X_test, y_test)\n",
        "model = train_network(CNN(), train_loader, val_loader, 10, 1e-3)\n",
        "accuracy = evaluate_network(model, test_loader)\n",
        "print(f'Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "TW26cz3ipNWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('========== Transfer Learning Resnet ==========')\n",
        "train_loader, val_loader, test_loader = load_torch_data(X_train, y_train, X_test, y_test)\n",
        "model = train_network(Resnet(), train_loader, val_loader, 10, 1e-3)\n",
        "accuracy = evaluate_network(model, test_loader)\n",
        "print(f'Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "AJHgtXPfJD_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('========== Transfer Learning VGG16 ==========')\n",
        "train_loader, val_loader, test_loader = load_torch_data(X_train, y_train, X_test, y_test)\n",
        "model = train_network(VGG16(), train_loader, val_loader, 10, 1e-3)\n",
        "accuracy = evaluate_network(model, test_loader)\n",
        "print(f'Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "IY5y1LM7fhqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('========== Transfer Learning EfficientNet ==========')\n",
        "train_loader, val_loader, test_loader = load_torch_data(X_train, y_train, X_test, y_test)\n",
        "model = train_network(EfficientNet(), train_loader, val_loader, 10, 1e-3)\n",
        "accuracy = evaluate_network(model, test_loader)\n",
        "print(f'Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "7cANkVqwmbCz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}