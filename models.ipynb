{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willjhliang/traffic-sign-recognition/blob/main/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRrDti8n2Vw8"
      },
      "outputs": [],
      "source": [
        "# Download dataset from github repo\n",
        "!rm -r sample_data\n",
        "!git clone https://github.com/willjhliang/traffic-sign-recognition.git\n",
        "!mv traffic-sign-recognition/data .\n",
        "!rm -r traffic-sign-recognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from copy import deepcopy\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.base import clone"
      ],
      "metadata": {
        "id": "bDwc2e3-4pvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 58                  # Number of classes\n",
        "S = 32                  # Size of image, dimension is (s, s, 3)\n",
        "validation_ratio = 0.1  # Proportion of training data to set aside for validation\n",
        "\n",
        "random_seed = 19104"
      ],
      "metadata": {
        "id": "8auRbcGP5jJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "FaaChV3Way94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(datapath):\n",
        "    data = {}\n",
        "    for k in range(K):\n",
        "        data[k] = []\n",
        "    for f in os.listdir(datapath):\n",
        "        k = int(f[:3])\n",
        "        img = Image.open(os.path.join(datapath, f)).convert('L')\n",
        "        img = np.asarray(img) / 255\n",
        "        data[k].append(img)\n",
        "    return data"
      ],
      "metadata": {
        "id": "Gr29Pwh625Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.read_csv(\"data/labels.csv\")\n",
        "\n",
        "train_data = load_data('data/images/train')\n",
        "test_data = load_data('data/images/test')"
      ],
      "metadata": {
        "id": "Br0-GXCA4rts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "\n",
        "We'll explore the dataset by displaying example images from each class. We also plot the number of images belonging to each class and find that it's extremely variable."
      ],
      "metadata": {
        "id": "MjD_ep2c5HUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.gray()\n",
        "fig, axs = plt.subplots(6, 10)\n",
        "fig.set_figheight(15)\n",
        "fig.set_figwidth(15)\n",
        "for k, (i, j) in itertools.zip_longest(range(K), list(itertools.product(range(6), range(10))), fillvalue=-1):\n",
        "    axs[i,j].axis('off')\n",
        "    if k >= 0:\n",
        "        axs[i,j].imshow(train_data[k][0])\n"
      ],
      "metadata": {
        "id": "MeVxBPI864JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_dist = [len(train_data[k]) for k in range(K)]\n",
        "plt.bar(list(range(K)), class_dist)"
      ],
      "metadata": {
        "id": "w3rniKpZ9gNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "To preprocess our data, we'll first augment the classes with fewer image examples. Our augmentation scheme includes cropping, rotation, and brightness changes; note that we don't apply any flips since it violates the symbols on traffic signs.\n",
        "\n",
        "After augmentation, we reshape the data to an array format and store labels as integers."
      ],
      "metadata": {
        "id": "kwHmRzn_WERd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def center_crop(img, center_percentage):\n",
        "    width, height = img.shape\n",
        "    width_offset = int(width * (1 - center_percentage) / 2)\n",
        "    height_offset = int(height * (1 - center_percentage) / 2)\n",
        "    img = img[width_offset:width-width_offset, height_offset:height-height_offset]\n",
        "    return img\n",
        "\n",
        "def rotate_img(img, angle):\n",
        "    height, width = img.shape\n",
        "    center_x, center_y = (width // 2, height // 2)\n",
        "\n",
        "    rot_mat = cv2.getRotationMatrix2D((center_x, center_y), angle, 1.0)\n",
        "    cos = np.abs(rot_mat[0, 0])\n",
        "    sin = np.abs(rot_mat[0, 1])\n",
        "\n",
        "    new_width = int((height * sin) + (width * cos))\n",
        "    new_height = int((height * cos) + (width * sin))\n",
        "    rot_mat[0, 2] += (new_width / 2) - center_x\n",
        "    rot_mat[1, 2] += (new_height / 2) - center_y\n",
        "\n",
        "    img = cv2.warpAffine(img, rot_mat, (new_width, new_height))\n",
        "    img = cv2.resize(img, (width, height))\n",
        "\n",
        "    return img\n",
        "\n",
        "def shift_brightness(img, shift):\n",
        "    img = np.clip(img + shift, 0, 255)\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "ETRb3zMX869y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "def augment_img(img):\n",
        "    rot_angle = randint(-30, 30)\n",
        "    crop_center_percentage = randint(70, 90) / 100\n",
        "    crop_center_percentage = 0.8\n",
        "    brightness_shift = randint(-20, 20) / 100\n",
        "    img = rotate_img(img, rot_angle)\n",
        "    img = center_crop(img, crop_center_percentage)\n",
        "    img = shift_brightness(img, brightness_shift)\n",
        "    img = center_crop(img, 0.8)\n",
        "    return img"
      ],
      "metadata": {
        "id": "D0LV8uZsCB0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "largest_class_size = max([len(train_data[k]) for k in range(K)])\n",
        "for k in range(K):\n",
        "    size_diff = largest_class_size - len(train_data[k])\n",
        "    for i in range(size_diff):\n",
        "        train_data[k].append(augment_img(train_data[k][i % len(train_data[k])]))"
      ],
      "metadata": {
        "id": "UG-NpBFR8hgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_class_dist = [len(train_data[k]) for k in range(K)]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "fig.set_figheight(15)\n",
        "fig.set_figwidth(15)\n",
        "axs[0].bar(list(range(K)), class_dist)\n",
        "axs[1].bar(list(range(K)), aug_class_dist)"
      ],
      "metadata": {
        "id": "nt5sG5QiHtzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data):\n",
        "    X = []\n",
        "    y = []\n",
        "    for k in range(K):\n",
        "        for i in data[k]:\n",
        "            i = cv2.resize(i, (S, S))\n",
        "            X.append(np.expand_dims(i, 0))\n",
        "            y.append(k)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    shuffled_indices = np.random.permutation(len(X))\n",
        "    X = X[shuffled_indices]\n",
        "    y = y[shuffled_indices]\n",
        "    X_flattened = np.reshape(X, (X.shape[0], -1))\n",
        "    \n",
        "    return X, X_flattened, y"
      ],
      "metadata": {
        "id": "4UVlLK8c3HXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_train_flattened, y_train = prepare_data(train_data)\n",
        "X_test, X_test_flattened, y_test = prepare_data(test_data)"
      ],
      "metadata": {
        "id": "MTT4G6q63fp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_validation(X_train, y_train):\n",
        "    val_split = int(X_train.shape[0] * validation_ratio)\n",
        "    X_train, X_val = X_train[val_split:], X_train[:val_split]\n",
        "    y_train, y_val = y_train[val_split:], y_train[:val_split]\n",
        "    return X_train, X_val, y_train, y_val"
      ],
      "metadata": {
        "id": "27w1N04HdH8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n",
        "\n",
        "The following is a set of models we run on the data. Starting with the most simple baseline K-Nearest Neighbors, we move toward more complex models.\n",
        "1. Baseline KNN\n",
        "2. Adaboost\n",
        "3. Logistic Regression\n",
        "4. Kernelized SVM\n",
        "5. Dense Neural Network\n",
        "6. Convolutional Neural Network\n",
        "\n",
        "We also test two more advanced strategies.\n",
        "1. Autoencoder dimensionality reduction allows us to embed the images in a lower dimensional space, which may lead to stronger classification performance by simpler models.\n",
        "2. Transfer learning with a CNN allows us to adapt weights from pre-trained networks to our traffic sign recognition problem."
      ],
      "metadata": {
        "id": "zUuXolGt5JZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline KNN"
      ],
      "metadata": {
        "id": "7WmDtprV5NqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a baseline K-Nearest Neighbors models to classify traffic sign images. Use 10-Fold cross validation to determine the best value of K"
      ],
      "metadata": {
        "id": "NCtWS2CqGgpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=10)\n",
        "\n",
        "def runKFold(model_base, X_train, y_train, one_only=False):\n",
        "   total_acc = 0\n",
        "   for train_index, val_index in kf.split(X_train): # Iterate through all 10 folds\n",
        "       # Split data into training data and validation data\n",
        "       X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "       y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        " \n",
        "       # Train model\n",
        "       model = clone(model_base)\n",
        "       model.fit(X_train_fold, y_train_fold)\n",
        "       total_acc += model.score(X_val_fold, y_val_fold)\n",
        "       if one_only:\n",
        "           return total_acc\n",
        "   avg_acc = total_acc / 10\n",
        "   return avg_acc"
      ],
      "metadata": {
        "id": "KEoZHVtJXuu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knnBaseline(X_train, y_train, X_test, y_test):\n",
        "    best_k = -1\n",
        "    best_acc = 0\n",
        "    val_accuracies = []\n",
        " \n",
        "    # Iterate through possible values of k from 1 to 30, incrementing by 2\n",
        "    for k_neighbors in range(1, 30, 2):\n",
        "        avg_acc = runKFold(KNeighborsClassifier(n_neighbors = k_neighbors), X_train, y_train)\n",
        "        val_accuracies.append(avg_acc)\n",
        "        if avg_acc > best_acc:\n",
        "            best_acc = avg_acc\n",
        "            best_k = k_neighbors\n",
        " \n",
        "    # Plot to show the best values\n",
        "    plt.plot(list(range(1, 30, 2)), val_accuracies)\n",
        "    plt.show()\n",
        "    print(\"Best k: \", best_k)\n",
        " \n",
        "    # Fit model with the best k value\n",
        "    model = KNeighborsClassifier(n_neighbors=best_k)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Accurcay on the test set\n",
        "    return model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "GokJxe4KGJz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(knnBaseline(X_train_flattened, y_train, X_test_flattened, y_test))"
      ],
      "metadata": {
        "id": "yGrcavdS2WX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adaboost"
      ],
      "metadata": {
        "id": "M1sX4gyWji8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adaboost(X_train, y_train, X_test, y_test):\n",
        "    X_train, X_val, y_train, y_val = get_validation(X_train, y_train)\n",
        "    learning_rates = np.linspace(0.1, 0.8, 8)\n",
        "    accs = []\n",
        "    for lr in learning_rates:\n",
        "        model = AdaBoostClassifier(\n",
        "            DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "            algorithm=\"SAMME.R\", learning_rate=lr, random_state=random_seed)\n",
        "        acc = runKFold(clf, X_train, y_train, True)\n",
        "        model.fit(X_train, y_train)\n",
        "        acc = model.score(X_val, y_val)\n",
        "        accs.append(acc)\n",
        "  \n",
        "    lr = learning_rates[np.argmax(accs)]\n",
        "    clf = AdaBoostClassifier(\n",
        "            DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "            algorithm=\"SAMME.R\", learning_rate=lr, random_state=random_seed)\n",
        "    clf.fit(X_train, y_train)\n",
        " \n",
        "    return clf.score(X_test, y_test)\n",
        " \n",
        "adaboost(X_train_flattened, y_train, X_test_flattened, y_test)\n"
      ],
      "metadata": {
        "id": "gdmToVHvYAvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "fgSsZgmhjr5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X_train, y_train, X_test, y_test):\n",
        "    Cvalues = [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0] # Values for how strong the regulation is\n",
        "    penalties = ['none', 'elasticnet', 'l1', 'l2'] # Different penalties for logistic regression\n",
        "    best_penalty = ''\n",
        "    best_C = -1\n",
        "    best_acc_ovr = 0\n",
        "    \n",
        "    for penalty in penalties:\n",
        "        val_accuracies = []\n",
        "        best_C_pen = -1\n",
        "        best_acc_inside = 0\n",
        "        for c in Cvalues:\n",
        "            # 10-fold CV\n",
        "            avg_acc = runKFold(LogisticRegression(penalty = penalty, C = c, multi_class = 'multinomial', solver = 'saga'), X_train, y_train)\n",
        "            val_accuracies.append(avg_acc)\n",
        "            if avg_acc > best_acc_inside:\n",
        "                best_acc_inside = avg_acc\n",
        "                best_C_pen = c\n",
        "            if avg_acc > best_acc_ovr:\n",
        "                best_acc_ovr = avg_acc\n",
        "                best_C = c\n",
        "                best_penalty = penalty\n",
        "        \n",
        "        # Plot the accuracies for the different C values for each penalty\n",
        "        plt.plot(Cvalues, val_accuracies)\n",
        "        plt.show()\n",
        "        print(\"Best C value for \", penalty, \"is \", best_C_pen)\n",
        " \n",
        "    # Get the best combination\n",
        "    print(\"Best C value and penalty overall \", best_C, \" \", best_penalty)\n",
        " \n",
        "    # Train new model with best combination\n",
        " \n",
        "    bestlogModel = LogisticRegression(penalty = best_penalty, C = best_C, multi_class = 'multinomial')\n",
        "    bestlogModel.fit(X_train, y_train)\n",
        "    return bestlogModel.score(X_test, y_test)\n",
        "\n",
        "logistic_regression(X_train_flattened, y_train, X_test_flattened, y_test)"
      ],
      "metadata": {
        "id": "KoQq0QNcX4T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernelized SVM"
      ],
      "metadata": {
        "id": "kBUqSumgjuwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        " \n",
        "def kernel_svm(X_train, y_train, X_test, y_test):\n",
        "   clf = SVC(kernel=\"linear\")\n",
        "   clf.fit(X_train, y_train)\n",
        "   return clf.score(X_test, y_test)\n",
        " \n",
        "kernel_svm(X_train_flattened, y_train, X_test_flattened, y_test)"
      ],
      "metadata": {
        "id": "iCLRPjXyYJOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense Neural Network"
      ],
      "metadata": {
        "id": "UPCS7NeYjxn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNeuralNet(nn.Module):\n",
        "    def __init__(self, input, hiddensize = 128):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input, hiddensize)\n",
        "        self.layer2 = nn.Linear(hiddensize, hiddensize)\n",
        "        self.out_layer = nn.Linear(hiddensize, 58)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.out_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kXZT0kN1c0JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network"
      ],
      "metadata": {
        "id": "xsattGC-jzYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv_1 = torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.max_pool2d = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv_2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.linear_1 = torch.nn.Linear(8 * 8 * 64, 128)\n",
        "        self.linear_2 = torch.nn.Linear(128, K)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv_1(x))\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.relu(self.conv_2(x))\n",
        "        x = self.max_pool2d(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.relu(self.linear_1(x))\n",
        "        x = self.linear_2(x)\n",
        "        return x\n",
        "        "
      ],
      "metadata": {
        "id": "4CB0H8YxKTnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor(X_train)\n",
        "\n",
        "train_set = torch.utils.data.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
        "test_set = torch.utils.data.TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=True)\n",
        "\n",
        "epochs = 10\n",
        "model = CNN()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) "
      ],
      "metadata": {
        "id": "GStjG8ksL4OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = []\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    for itr, (image, label) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        y_predicted = model(image)\n",
        "        label = label.long()\n",
        "        loss = criterion(y_predicted, label.long())\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss.append(running_loss)\n",
        "    print(f'epoch: {epoch+1}, loss: {running_loss:.4f}')"
      ],
      "metadata": {
        "id": "ICIMEyzpN30c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for itr, (image, label) in enumerate(test_loader):\n",
        "        outputs = model(image)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += predicted.eq(label.reshape(len(label),)).sum() \n",
        "        total += float(len(label))\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy of Neural Network is {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "JrWU3nUwTRAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder Dimensionality Reduction"
      ],
      "metadata": {
        "id": "DGWm6fptj051"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adaboost"
      ],
      "metadata": {
        "id": "KQy6vySzkBKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "-HSnHQVsj9Jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kernelized SVM"
      ],
      "metadata": {
        "id": "tTSK437qj_dZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning with CNN"
      ],
      "metadata": {
        "id": "jI9lT-q-kteU"
      }
    }
  ]
}